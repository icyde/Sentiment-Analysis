{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **50.007 ML 1D Project**\n",
    "By Group 12:\n",
    "\n",
    "1003424 Ooi Jia Sheng\n",
    "\n",
    "1005983 Brandon Tan Rui En\n",
    "\n",
    "1006340 Darren Chan Yu Hao\t\n",
    "\n",
    "1006240 Michelle Chrisalyn Djunaidi\t\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "np.random.seed(1993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read data\n",
    "\n",
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            results.append(line.strip())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read dev.out data\n",
    "def read_dev_out_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read train data\n",
    "def read_train_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdath of the data\n",
    "#------------------------------------\n",
    "# Spanish: ES\n",
    "ES_dev_in_data_path = os.path.join(\"Data\", \"ES\" , \"dev.in\")\n",
    "ES_dev_out_data_path = os.path.join(\"Data\", \"ES\" , \"dev.out\")\n",
    "ES_train_data_path = os.path.join(\"Data\", \"ES\" , \"train\")\n",
    "\n",
    "# Russiadn: RU\n",
    "RU_dev_in_data_path = os.path.join(\"Data\", \"RU\" , \"dev.in\")\n",
    "RU_dev_out_data_path = os.path.join(\"Data\", \"RU\" , \"dev.out\")\n",
    "RU_train_data_path = os.path.join(\"Data\", \"RU\" , \"train\")\n",
    "#------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words and tags\n",
    "def split_words_tags(labeled_data):\n",
    "        words = []\n",
    "        tags = []\n",
    "\n",
    "        for word_tag in labeled_data:\n",
    "            \n",
    "            if len(word_tag) != 2:\n",
    "                continue\n",
    "            \n",
    "            #word_tag is a list\n",
    "            word = word_tag[0]\n",
    "            tag = word_tag[1]\n",
    "\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "\n",
    "        return words, tags\n",
    "\n",
    "# Count unique tags\n",
    "def count_unique_tags(tags_ls):\n",
    "\n",
    "    tags_unique = set()\n",
    "    for tag in tags_ls:\n",
    "        tags_unique.add(tag)\n",
    "    return tags_unique\n",
    "\n",
    "# Count unique words\n",
    "def count_unique_words(words_ls):\n",
    "         \n",
    "    words_unique = set()\n",
    "    for word in words_ls:\n",
    "        words_unique.add(word)\n",
    "    return words_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Parameters\n",
    "\n",
    "# Get the emission parameters\n",
    "def get_emission_parameters(ls_of_tags, ls_of_words, tags, words, k=1):\n",
    "\n",
    "  # Write a function that estimates the emission parameters from the training set using MLE (maximumlikelihood estimation):\n",
    "    # e(x|y) = Count(y -> x) / Count(y)\n",
    "    # Count(y -> x) = Number of times word x is tagged with tag y\n",
    "    # Count(y) = Number of times tag y appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unqiue tags\n",
    "    # Input: ls_of_words - list of unqiue words\n",
    "    # Input: tags - list of all tags\n",
    "    # Input: words - list of all words\n",
    "    # Output: emission_parameters\n",
    "\n",
    "    # emission_parameters is a dictionary where:\n",
    "        # The keys are (tag, word) tuples\n",
    "        # The values are the emission parameters e(x|y)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"apple\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"apple\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    emission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (tag, word) tuple\n",
    "    count_y_to_x = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "    \n",
    "    print(f\"This is Count(y) : {count_y}\")\n",
    "\n",
    "    # Get the count of each (tag, word) tuple from the training set\n",
    "    for tag, word in zip(tags, words):\n",
    "        if (tag, word) in count_y_to_x:\n",
    "            count_y_to_x[(tag, word)] += 1\n",
    "        else:\n",
    "            count_y_to_x[(tag, word)] = 1\n",
    "\n",
    "    print(f\"This is Count(y -> x) : {count_y_to_x}\")\n",
    "\n",
    "    # Get the emission parameters\n",
    "    for tag, word in count_y_to_x:\n",
    "\n",
    "        emission_parameters[(tag, word)] = count_y_to_x[(tag, word)] / (count_y[tag] + k)\n",
    "\n",
    "    # For words that do not appear in the training set, k/(Count(y)+k) is used as the emission parameter\n",
    "    unknown_word = \"UNK\"\n",
    "    for tag in count_y:\n",
    "        emission_parameters[(tag, unknown_word)] = k / (count_y[tag] + k)\n",
    "\n",
    "    print(f\"This is e(x|y) : {emission_parameters}\")\n",
    "\n",
    "    return emission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_estimate_tags(test_words, emission_params, train_ls_of_words):\n",
    "\n",
    "    # for each word in the test set of words (test_words) assign the tag with the highest emission probability\n",
    "\n",
    "    # Inputs : test_tags - a list of all tags\n",
    "    #          test_ls_of_tags - a list of unqiue tags\n",
    "    #         test_number_of_tags - a list of the number of tags\n",
    "    #        test_words - a list of all words\n",
    "    #       emission_params - a dictionary of emission parameters\n",
    "    # \n",
    "    # Output : labelled words - a list of words with their assigned tags\n",
    "\n",
    "    predicted_results = []\n",
    "\n",
    "    for word in test_words:\n",
    "        if word in train_ls_of_words:\n",
    "\n",
    "            # y∗ = arg max y e(x|y)\n",
    "            emission_value = 0\n",
    "            for key in emission_params:\n",
    "                if key[1] == word:\n",
    "                    if emission_value < emission_params[key]:\n",
    "                        emission_value = emission_params[key]\n",
    "                        value = key[0]\n",
    "            \n",
    "            predicted_results.append((word, value))\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if word != \"\":\n",
    "                # y∗ = arg max y e(x|y)\n",
    "                emission_value = 0\n",
    "                for key in emission_params:\n",
    "                    if key[1] == \"UNK\":\n",
    "                        if emission_value < emission_params[key]:\n",
    "                            emission_value = emission_params[key]\n",
    "                            value = key[0]\n",
    "\n",
    "                predicted_results.append((\"UNK\", value))\n",
    "            \n",
    "            else:\n",
    "                predicted_results.append((\"\", \"\"))\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"predicted_results: \", predicted_results)\n",
    "    return predicted_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_part_1(dev_in_data_path,train_data_path, output_path):\n",
    "\n",
    "    train_data = read_train_data(train_data_path)\n",
    "\n",
    "    train_words, train_tags = split_words_tags(train_data)\n",
    "    train_ls_of_tags = count_unique_tags(train_tags)\n",
    "    train_ls_of_words = count_unique_words(train_words)\n",
    "\n",
    "    # Get Emmission Parameters\n",
    "    k = 1\n",
    "    emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)\n",
    "\n",
    "    test_data = read_dev_in_data(dev_in_data_path)\n",
    "\n",
    "    # Get labels for test data\n",
    "    test_labels = assign_estimate_tags(test_data, emission_params, train_ls_of_words)\n",
    "\n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for line in test_labels:\n",
    "            write_line = line[0] + \" \" + line[1] + \"\\n\"\n",
    "            file.write(write_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Spanish: \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/ES/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFor Spanish: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m output_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mData\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mES\u001b[39m\u001b[39m\"\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mdev.p1.out\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m calculate_part_1(ES_dev_in_data_path, ES_train_data_path, output_path)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# For Russian\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mcalculate_part_1\u001b[0;34m(dev_in_data_path, train_data_path, output_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_part_1\u001b[39m(dev_in_data_path,train_data_path, output_path):\n\u001b[0;32m----> 3\u001b[0m     train_data \u001b[39m=\u001b[39m read_train_data(train_data_path)\n\u001b[1;32m      5\u001b[0m     train_words, train_tags \u001b[39m=\u001b[39m split_words_tags(train_data)\n\u001b[1;32m      6\u001b[0m     train_ls_of_tags \u001b[39m=\u001b[39m count_unique_tags(train_tags)\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mread_train_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_train_data\u001b[39m(filepath):\n\u001b[1;32m     28\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filepath, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     31\u001b[0m         lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     32\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/ES/train'"
     ]
    }
   ],
   "source": [
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p1.out\")\n",
    "calculate_part_1(ES_dev_in_data_path, ES_train_data_path, output_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print(\"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"dev.p1.out\")\n",
    "calculate_part_1(RU_dev_in_data_path, RU_train_data_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Viterbi Algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data_p2(filepath):\n",
    "    #Appends Start and STOP state so that we can do viterbi\n",
    "    results = []\n",
    "    #Add start state\n",
    "    results.append(' START')\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            results.append(stripped_line)\n",
    "            if stripped_line == \"\":\n",
    "                # include stop and start states at new sentence\n",
    "                results.append(' STOP')\n",
    "                results.append(' START')\n",
    "    final_results = []\n",
    "    for line in results:\n",
    "        split_lines = line.split(\" \")\n",
    "        final_results.append(split_lines)\n",
    "    final_results.pop()\n",
    "    #remove final start state\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transmission_parameters(ls_of_tags, tags):\n",
    "    #Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation)\n",
    "    # q( y_i | y_i-1 ) = Count( y_i-1, y_i ) / Count( y_i-1 )\n",
    "    # Count(y_i-1 , y_i) = Number of times tag y_i-1 transits to tag y_i\n",
    "    # Count(y_i-1) = Number of times tag y_i-1 appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unique tags\n",
    "    # Input: tags - list of all tags\n",
    "\n",
    "    # transmission_parameters is a dictionary where:\n",
    "        # The keys are (tag_y_i-1, tag_y_i) tuples\n",
    "        # The values are the transmission parameters q(y_i | y_i-1)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"O\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"O\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    transmission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (y_i-1, y_i) tuple\n",
    "    count_y_i_1_to_y_i = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "\n",
    "    print(f\"This is Count(y) : {count_y}\")\n",
    "\n",
    "    # Get the count of each (y_i-1, y_i) tuple from the training set\n",
    "    for i in range(1, len(tags)):\n",
    "        if (tags[i-1], tags[i]) in count_y_i_1_to_y_i:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] +=1\n",
    "        else:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] =1\n",
    "\n",
    "    print(f\"This is Count (y_i-1 , y_i) : {count_y_i_1_to_y_i}\")\n",
    "\n",
    "    #transmission probability from state y_i-1 to y_i e.g (\"START\", \"O\") = 0.9281 == 0.9281 probability to transmit from \"START\" to \"O\" state\n",
    "\n",
    "    for key, value in count_y_i_1_to_y_i.items():\n",
    "        transmission_parameters[key] = value / count_y[key[0]]\n",
    "\n",
    "    print(f\"This is the q(y_i | y_i-1): {transmission_parameters}\")\n",
    "\n",
    "    labels = [\"START\", \"STOP\", \"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\"]\n",
    "    for i in labels:\n",
    "        for j in labels:\n",
    "            if (i, j) in transmission_parameters:\n",
    "                continue\n",
    "            else:\n",
    "                transmission_parameters[(i, j)] = 0\n",
    "\n",
    "    return transmission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that if we just blindly shove the fractions into the algorithm\n",
    "# multiply fractions enough times and it'll approach 0\n",
    "# and yeah that's gonna end up becoming 0 ft. computer inaccuracy\n",
    "# that's the numerical underflow\n",
    "# we can prevent this by log-ing everything\n",
    "\n",
    "# both transition_parameters and emission_parameters are dictionaries\n",
    "\n",
    "def log_underflow_prevention(parameter_dict):\n",
    "    log_parameter_dict = {}\n",
    "    for key, value in parameter_dict.items():\n",
    "        if value <0 :\n",
    "            log_parameter_dict[key] = -np.inf\n",
    "        elif value == 0:\n",
    "            log_parameter_dict[key] = -np.inf\n",
    "        else:\n",
    "            log_parameter_dict[key] = np.log(value)\n",
    "    return log_parameter_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the dev data output list of list\n",
    "def read_dev(path):\n",
    "  out = [[]]\n",
    "  f = open(path, \"r\", encoding=\"utf-8\")\n",
    "  lines_in = f.readlines()\n",
    "  for word in lines_in:\n",
    "    if word == \"\\n\":\n",
    "      out.append([])\n",
    "    else:\n",
    "      out[-1].append(word.rstrip())\n",
    "  return out[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(document, transmission, emission, ls_of_words):\n",
    "  n = len(document)\n",
    "  tags = [\"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\",\"STOP\"]\n",
    "\n",
    "  memo = [{} for _ in range(n+1)]\n",
    "  parent_arr = [{} for _ in range(n+1)]\n",
    "  #initial step from start to first node\n",
    "  for tag in tags:\n",
    "    a_v_u = transmission.get((\"START\", tag)) \n",
    "    if document[0] in ls_of_words:\n",
    "      # if tag emits word, get emission, else -inf\n",
    "      b_u = emission.get((tag, document[0])) or -np.inf\n",
    "    else:\n",
    "      #if word not in document\n",
    "      b_u = emission.get((tag, \"UNK\"))  or -np.inf\n",
    "    memo[0][tag] =  a_v_u + b_u\n",
    "    parent_arr[0][tag] = None\n",
    "  #recursive\n",
    "  for j in range(1,n):\n",
    "    for u in tags:\n",
    "      max_prob = -np.inf\n",
    "      max_v = None\n",
    "      for v in tags:\n",
    "        if document[j] in ls_of_words:\n",
    "          emission_prob = emission.get((u, document[j])) or -np.inf\n",
    "        else:\n",
    "          emission_prob = emission.get((u, \"UNK\")) or -np.inf\n",
    "        transmission_v_u = transmission.get((v, u)) or -np.inf \n",
    "        prob = memo[j-1][v] + transmission_v_u+ emission_prob\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_v = v\n",
    "      memo[j][u] = max_prob\n",
    "      parent_arr[j][u] = max_v\n",
    "  # Termination step\n",
    "\n",
    "  max_prob = -np.inf\n",
    "  max_v = None\n",
    "  for tag in tags:\n",
    "    prob = memo[n-1][tag] + transmission.get((tag, \"STOP\"))\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_v = tag\n",
    "\n",
    "  if max_prob != -np.inf:\n",
    "    memo[n]['STOP'] = max_prob\n",
    "    parent_arr[n]['STOP'] = max_v\n",
    "    \n",
    "  most_likely_sequence = [\"\" for _ in range(n)]\n",
    "  if max_v == None:\n",
    "        max_v = \"O\"\n",
    "  # Backtrack to find the most likely path\n",
    "  for t in range(n , 0, -1):\n",
    "    max_v = parent_arr[t].get(max_v)\n",
    "    if max_v == None:\n",
    "      max_v = \"O\"\n",
    "    most_likely_sequence[t-1] = max_v\n",
    "\n",
    "  return most_likely_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_loop(data, transmission, emission, ls_of_words):\n",
    "  results =[]\n",
    "  for document in data:\n",
    "    results.append(viterbi(document, transmission, emission, ls_of_words))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the prediction from trained data into the dev.in file and output\n",
    "def assign_prediction(prediction, data, path):\n",
    "    if (len(prediction) != len(data)):\n",
    "        return \"Error\"\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        assert( len(prediction[i])== len(data[i]))\n",
    "        m = len(data[i])\n",
    "        for j in range(m):\n",
    "            file.write(data[i][j] + \" \" + prediction[i][j] + \"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "    print(\"Wrote predictions to\", path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_part_2(dev_in_data_path, train_data_path, output_path):\n",
    "  #sort train data into tag and words\n",
    "  train_data = read_train_data(train_data_path)\n",
    "  train_words, train_tags = split_words_tags(train_data)\n",
    "  train_ls_of_tags = count_unique_tags(train_tags)\n",
    "  train_ls_of_words = count_unique_words(train_words)\n",
    "  # Get Emission Parameters\n",
    "  k = 1\n",
    "  emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)\n",
    "\n",
    "  #append start and stop to train data per document\n",
    "  train_data_modified = read_train_data_p2(train_data_path)\n",
    "  train_words_modified, train_tags_modified = split_words_tags(train_data_modified)\n",
    "  train_ls_of_tags_modified = count_unique_tags(train_tags_modified)\n",
    "  transmission_params = get_transmission_parameters(train_ls_of_tags_modified, train_tags_modified)\n",
    "\n",
    "  #log transmission and emission params to avoid underflow\n",
    "  log_emission = log_underflow_prevention(emission_params)\n",
    "  log_transmission = log_underflow_prevention(transmission_params)\n",
    "  # read dev_in in list of list\n",
    "  dev_in_list = read_dev(dev_in_data_path)\n",
    "  \n",
    "  # run viterbi and get predictions for the whole dev_in\n",
    "  predictions = viterbi_loop(dev_in_list, log_transmission, log_emission, train_ls_of_words)\n",
    "  # write predictions into dev.p2.out\n",
    "  assign_prediction(predictions, dev_in_list, output_path)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_part_2(dev_in_data_path,train_data_path, output_path):\n",
    "  write_predictions_part_2(dev_in_data_path,train_data_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p2.out\")\n",
    "calculate_part_2(ES_dev_in_data_path, ES_train_data_path, output_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print(\"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"dev.p2.out\")\n",
    "calculate_part_2(RU_dev_in_data_path, RU_train_data_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Top k-best viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_best_viterbi(document, transmission, emission, ls_of_words, k):\n",
    "    n = len(document)\n",
    "    tags = [\"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\"]\n",
    "    # store k-best paths\n",
    "    k_best_paths = [(\"START\", 0, []) for _ in range(k)]\n",
    "\n",
    "    for i in range(0, n):\n",
    "        new_k_best_paths = []\n",
    "        for tag, path_prob, prev_path in k_best_paths:\n",
    "            for next_tag in tags:  \n",
    "              a_uv = transmission.get((tag, next_tag)) \n",
    "              if document[i] in ls_of_words:\n",
    "                b_uo = emission.get((next_tag, document[i])) or -np.inf\n",
    "              else:\n",
    "                b_uo = emission.get((next_tag, \"UNK\")) or -np.inf\n",
    "              new_prob = path_prob + a_uv + b_uo\n",
    "              if (i==0):\n",
    "                 new_k_best_paths.append((next_tag, new_prob, []))\n",
    "              elif (next_tag, new_prob, prev_path + [tag]) not in new_k_best_paths:\n",
    "                new_k_best_paths.append((next_tag, new_prob, prev_path + [tag]))\n",
    "        \n",
    "        #sort top 8, drop the rest\n",
    "        new_k_best_paths.sort(key=lambda x: x[1], reverse=True)\n",
    "        k_best_paths = new_k_best_paths[:k]\n",
    "\n",
    "    #termination step:\n",
    "    final_k_best_paths = []\n",
    "\n",
    "    for tag, path_prob, prev_path in k_best_paths:\n",
    "        final_prob = path_prob + transmission.get((tag, \"STOP\"))\n",
    "        final_k_best_paths.append((tag, final_prob, prev_path + [tag]))\n",
    "    \n",
    "    final_k_best_paths.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Backtracking\n",
    "    kth_best_path = final_k_best_paths\n",
    "    if len(kth_best_path) >= 8: \n",
    "        # get 2nd and 8th best sequence\n",
    "        second = kth_best_path[1]\n",
    "        eighth = kth_best_path[7]\n",
    "    else:\n",
    "        # Get last output sequence if list is shorter than 8\n",
    "        second = kth_best_path[1]\n",
    "        eighth = kth_best_path[-1]\n",
    "    \n",
    "    return [second, eighth]  # The k-th best path (list of states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_part_3(dev_in_data_path, dev_out_2nd_data_path, dev_out_8th_data_path, train_data_path):\n",
    "  #sort train data into tag and words\n",
    "  train_data = read_train_data(train_data_path)\n",
    "  train_words, train_tags = split_words_tags(train_data)\n",
    "  train_ls_of_tags = count_unique_tags(train_tags)\n",
    "  train_ls_of_words = count_unique_words(train_words)\n",
    "  # Get Emission Parameters\n",
    "  emission_k = 1\n",
    "  emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, emission_k)\n",
    "\n",
    "  #append start and stop to train data per document\n",
    "  train_data_modified = read_train_data_p2(train_data_path)\n",
    "  train_words_modified, train_tags_modified = split_words_tags(train_data_modified)\n",
    "  train_ls_of_tags_modified = count_unique_tags(train_tags_modified)\n",
    "  transmission_params = get_transmission_parameters(train_ls_of_tags_modified, train_tags_modified)\n",
    "\n",
    "  #log transmission and emission params to avoid underflow\n",
    "  log_emission = log_underflow_prevention(emission_params)\n",
    "  log_transmission = log_underflow_prevention(transmission_params)\n",
    "  # read dev_in in list of list\n",
    "  dev_in_list = read_dev(dev_in_data_path)\n",
    "  \n",
    "  # run kth-best-viterbi_loop and get predictions for the whole dev_in, store best 8 sequences\n",
    "  k = 8\n",
    "  total_second_seq = []\n",
    "  total_eighth_seq = []\n",
    "  for data in dev_in_list:\n",
    "    predictions = k_best_viterbi(data, log_transmission, log_emission, train_ls_of_words, k)\n",
    "    # print(\"2nd best sequence: \", \"prob:\", predictions[0][1], \"\\n sequence: \", predictions[0][2])\n",
    "    # print(\"8th best sequence: \", \"prob:\", predictions[1][1], \"\\n sequence: \", predictions[1][2])\n",
    "\n",
    "    #tags for 2nd best sequence\n",
    "    second_seq = predictions[0][2]\n",
    "    total_second_seq.append(second_seq)\n",
    "    #tags for 8th best sequence\n",
    "    eighth_seq = predictions[1][2]\n",
    "    total_eighth_seq.append(eighth_seq)\n",
    "  \n",
    "  # write predictions into dev.p3.2nd.out and dev.p3.8th.out\n",
    "  assign_prediction(total_second_seq, dev_in_list, dev_out_2nd_data_path)\n",
    "  assign_prediction(total_eighth_seq, dev_in_list, dev_out_8th_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_dev_out_p3_2nd_data_path = os.path.join(\"Data\", \"ES\" , \"dev.p3.2nd.out\")\n",
    "ES_dev_out_p3_8th_data_path = os.path.join(\"Data\", \"ES\" , \"dev.p3.8th.out\")\n",
    "RU_dev_out_p3_2nd_data_path = os.path.join(\"Data\", \"RU\" , \"dev.p3.2nd.out\")\n",
    "RU_dev_out_p3_8th_data_path = os.path.join(\"Data\", \"RU\" , \"dev.p3.8th.out\")\n",
    "\n",
    "\n",
    "write_predictions_part_3(ES_dev_in_data_path, ES_dev_out_p3_2nd_data_path, ES_dev_out_p3_8th_data_path, ES_train_data_path)\n",
    "print(\"\\n\")\n",
    "write_predictions_part_3(RU_dev_in_data_path, RU_dev_out_p3_2nd_data_path, RU_dev_out_p3_8th_data_path, RU_train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Design Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read data\n",
    "\n",
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            results.append(line.strip())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read dev.out data\n",
    "def read_dev_out_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read train data\n",
    "def read_train_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words and tags\n",
    "def split_words_tags(labeled_data):\n",
    "        words = []\n",
    "        tags = []\n",
    "\n",
    "        for word_tag in labeled_data:\n",
    "            \n",
    "            if len(word_tag) != 2:\n",
    "                continue\n",
    "            \n",
    "            #word_tag is a list\n",
    "            word = word_tag[0]\n",
    "            tag = word_tag[1]\n",
    "\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "\n",
    "        return words, tags\n",
    "\n",
    "# Count unique tags\n",
    "def count_unique_tags(tags_ls):\n",
    "\n",
    "    tags_unique = set()\n",
    "    for tag in tags_ls:\n",
    "        tags_unique.add(tag)\n",
    "    return tags_unique\n",
    "\n",
    "# Count unique words\n",
    "def count_unique_words(words_ls):\n",
    "         \n",
    "    words_unique = set()\n",
    "    for word in words_ls:\n",
    "        words_unique.add(word)\n",
    "    return words_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Parameters\n",
    "\n",
    "# Get the emission parameters\n",
    "def get_emission_parameters(ls_of_tags, ls_of_words, tags, words, k=1):\n",
    "\n",
    "  # Write a function that estimates the emission parameters from the training set using MLE (maximumlikelihood estimation):\n",
    "    # e(x|y) = Count(y -> x) / Count(y)\n",
    "    # Count(y -> x) = Number of times word x is tagged with tag y\n",
    "    # Count(y) = Number of times tag y appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unqiue tags\n",
    "    # Input: ls_of_words - list of unqiue words\n",
    "    # Input: tags - list of all tags\n",
    "    # Input: words - list of all words\n",
    "    # Output: emission_parameters\n",
    "\n",
    "    # emission_parameters is a dictionary where:\n",
    "        # The keys are (tag, word) tuples\n",
    "        # The values are the emission parameters e(x|y)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"apple\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"apple\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    emission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (tag, word) tuple\n",
    "    count_y_to_x = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "    \n",
    "    # Get the count of each (tag, word) tuple from the training set\n",
    "    for tag, word in zip(tags, words):\n",
    "        if (tag, word) in count_y_to_x:\n",
    "            count_y_to_x[(tag, word)] += 1\n",
    "        else:\n",
    "            count_y_to_x[(tag, word)] = 1\n",
    "\n",
    "    # Get the emission parameters\n",
    "    for tag, word in count_y_to_x:\n",
    "\n",
    "        emission_parameters[(tag, word)] = count_y_to_x[(tag, word)] / (count_y[tag] + k) # SOMETHING WRONG WITH THIS FORMULA\n",
    "\n",
    "    # For words that do not appear in the training set, k/(Count(y)+k) is used as the emission parameter\n",
    "    unknown_word = \"UNK\"\n",
    "    for tag in count_y:\n",
    "        emission_parameters[(tag, unknown_word)] = k / (count_y[tag] + k)\n",
    "\n",
    "    return emission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_estimate_tags(test_words, emission_params, train_ls_of_words):\n",
    "\n",
    "    # for each word in the test set of words (test_words) assign the tag with the highest emission probability\n",
    "\n",
    "    # Inputs : test_tags - a list of all tags\n",
    "    #          test_ls_of_tags - a list of unqiue tags\n",
    "    #         test_number_of_tags - a list of the number of tags\n",
    "    #        test_words - a list of all words\n",
    "    #       emission_params - a dictionary of emission parameters\n",
    "    # \n",
    "    # Output : labelled words - a list of words with their assigned tags\n",
    "\n",
    "    predicted_results = []\n",
    "\n",
    "    for word in test_words:\n",
    "        if word in train_ls_of_words:\n",
    "\n",
    "            # y∗ = arg max y e(x|y)\n",
    "            emission_value = 0\n",
    "            for key in emission_params:\n",
    "                if key[1] == word:\n",
    "                    if emission_value < emission_params[key]:\n",
    "                        emission_value = emission_params[key]\n",
    "                        value = key[0]\n",
    "            \n",
    "            predicted_results.append((word, value))\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if word != \"\":\n",
    "                # y∗ = arg max y e(x|y)\n",
    "                emission_value = 0\n",
    "                for key in emission_params:\n",
    "                    if key[1] == \"UNK\":\n",
    "                        if emission_value < emission_params[key]:\n",
    "                            emission_value = emission_params[key]\n",
    "                            value = key[0]\n",
    "\n",
    "                predicted_results.append((\"UNK\", value))\n",
    "            \n",
    "            else:\n",
    "                predicted_results.append((\"\", \"\"))\n",
    "\n",
    "    return predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words and tags\n",
    "def split_character_tags(labeled_data):\n",
    "        words = []\n",
    "        tags = []\n",
    "        character = []\n",
    "        length_num_of_words = []\n",
    "\n",
    "        for word_tag in labeled_data:\n",
    "            \n",
    "            if len(word_tag) != 2:\n",
    "                continue\n",
    "            \n",
    "            # #word_tag is a list\n",
    "            # word = word_tag[0]\n",
    "            # tag = word_tag[1]\n",
    "\n",
    "            # words.append(word)\n",
    "            # tags.append(tag)\n",
    "\n",
    "            # split word into characters\n",
    "            for character in word_tag[0]:\n",
    "                words.append(character)\n",
    "                tags.append(word_tag[1])\n",
    "                length_num_of_words.append(len(word_tag[0])) # To combine back the words later\n",
    "            \n",
    "        return words, tags, length_num_of_words\n",
    "\n",
    "def count_unique_characters(character):\n",
    "    char_unique = set()\n",
    "    for unqiue_character in character:\n",
    "        char_unique.add(unqiue_character)\n",
    "    return char_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Parameters\n",
    "\n",
    "# Get the emission parameters\n",
    "def get_emission_parameters_char(ls_of_char, ls_of_tags, tags, chars, k=1):\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    emission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (tag, word) tuple\n",
    "    count_y_to_x = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "\n",
    "    # Get the count of each (tag, word) tuple from the training set\n",
    "    for tag, char in zip(tags, chars):\n",
    "        if (tag, char) in count_y_to_x:\n",
    "            count_y_to_x[(tag, char)] += 1\n",
    "        else:\n",
    "            count_y_to_x[(tag, char)] = 1\n",
    "\n",
    "    # Get the emission parameters\n",
    "    for tag, char in count_y_to_x:\n",
    "        emission_parameters[(tag, char)] = count_y_to_x[(tag, char)] / (count_y[tag] + k)\n",
    "\n",
    "    # For charc that do not appear in the training set, k/(Count(y)+k) is used as the emission parameter\n",
    "    unknown_word = \"UNK\"\n",
    "    for tag in count_y:\n",
    "        emission_parameters[(tag, unknown_word)] = k / (count_y[tag] + k)\n",
    "\n",
    "    return emission_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transmission Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data_p2(filepath):\n",
    "    results = []\n",
    "    #Add start state\n",
    "    results.append(' START')\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            results.append(stripped_line)\n",
    "            if stripped_line == \"\":\n",
    "                # include stop and start states at new sentence\n",
    "                results.append(' STOP')\n",
    "                results.append(' START')\n",
    "    final_results = []\n",
    "    for line in results:\n",
    "        split_lines = line.split(\" \")\n",
    "        final_results.append(split_lines)\n",
    "    final_results.pop()\n",
    "    #remove final start state\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transmission_parameters(ls_of_tags, tags):\n",
    "    #Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation)\n",
    "    # q( y_i | y_i-1 ) = Count( y_i-1, y_i ) / Count( y_i-1 )\n",
    "    # Count(y_i-1 , y_i) = Number of times tag y_i-1 transits to tag y_i\n",
    "    # Count(y_i-1) = Number of times tag y_i-1 appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unique tags\n",
    "    # Input: tags - list of all tags\n",
    "\n",
    "    # transmission_parameters is a dictionary where:\n",
    "        # The keys are (tag_y_i-1, tag_y_i) tuples\n",
    "        # The values are the transmission parameters q(y_i | y_i-1)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"O\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"O\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    transmission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (y_i-1, y_i) tuple\n",
    "    count_y_i_1_to_y_i = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "\n",
    "    # Get the count of each (y_i-1, y_i) tuple from the training set\n",
    "    for i in range(1, len(tags)):\n",
    "        if (tags[i-1], tags[i]) in count_y_i_1_to_y_i:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] +=1\n",
    "        else:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] =1\n",
    "\n",
    "    #transmission probability from state y_i-1 to y_i e.g (\"START\", \"O\") = 0.9281 == 0.9281 probability to transmit from \"START\" to \"O\" state\n",
    "\n",
    "    for key, value in count_y_i_1_to_y_i.items():\n",
    "        transmission_parameters[key] = value / count_y[key[0]]\n",
    "\n",
    "    labels = [\"START\", \"STOP\", \"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\"]\n",
    "    for i in labels:\n",
    "        for j in labels:\n",
    "            if (i, j) in transmission_parameters:\n",
    "                continue\n",
    "            else:\n",
    "                transmission_parameters[(i, j)] = 0\n",
    "\n",
    "    return transmission_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "import random\n",
    "\n",
    "\n",
    "def viterbi(document, transmission, emission, emission_char, chars, ls_of_words):\n",
    "  n = len(document)\n",
    "  tags = [\"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\",\"STOP\"]\n",
    "\n",
    "  memo = [{} for _ in range(n+1)]\n",
    "  parent_arr = [{} for _ in range(n+1)]\n",
    "  #initial step from start to first node\n",
    "  for tag in tags:\n",
    "    a_v_u = transmission.get((\"START\", tag)) \n",
    "    if document[0] in ls_of_words:\n",
    "      # if tag emits word, get emission, else -inf\n",
    "      b_u = emission.get((tag, document[0])) or -np.inf\n",
    "    else:\n",
    "      # #if word not in document\n",
    "      # b_u = emission.get((tag, \"UNK\"))  or -np.inf\n",
    "      b_u_char = []\n",
    "      for char in document[0]:\n",
    "        if (char in chars):\n",
    "          b_u_char.append(emission_char.get((tag, char)) or -np.inf)\n",
    "        else:\n",
    "          b_u_char.append(emission_char.get((tag, \"UNK\")) or -np.inf)\n",
    "      \n",
    "      # Method 1 : Find most occured tag for b_u_char\n",
    "      #b_u = Counter(b_u_char).most_common(1)[0][1]\n",
    "\n",
    "      # Method 2 : Use probability of each tag for b_u_char and randomly select one\n",
    "      b_u = random.choices(b_u_char).pop()\n",
    "\n",
    "    memo[0][tag] =  a_v_u + b_u\n",
    "    parent_arr[0][tag] = None\n",
    "\n",
    "  #recursive\n",
    "  for j in range(1,n):\n",
    "    for u in tags:\n",
    "      max_prob = -np.inf\n",
    "      max_v = None\n",
    "      for v in tags:\n",
    "        if document[j] in ls_of_words:\n",
    "          emission_prob = emission.get((u, document[j])) or -np.inf\n",
    "        # else:\n",
    "        #   emission_prob = emission.get((u, \"UNK\")) or -np.inf\n",
    "        else:\n",
    "          # #if word not in document\n",
    "          # b_u = emission.get((tag, \"UNK\"))  or -np.inf\n",
    "          emission_prob_char = []\n",
    "          for char in document[j]:\n",
    "            if (char in chars):\n",
    "              emission_prob_char.append(emission_char.get((u, char)) or -np.inf)\n",
    "            else:\n",
    "              emission_prob_char.append(emission_char.get((u, \"UNK\")) or -np.inf)\n",
    "\n",
    "          # # Method 1 : Find most occured tag for emission_prob_char\n",
    "          # emission_prob = Counter(emission_prob_char).most_common(1)[0][1]\n",
    "\n",
    "           # Method 2 : Use probability of each tag for emission_prob_char and randomly select one\n",
    "          emission_prob = random.choices(emission_prob_char).pop()\n",
    "        \n",
    "        transmission_v_u = transmission.get((v, u)) or -np.inf \n",
    "        prob = memo[j-1][v] + transmission_v_u+ emission_prob\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_v = v\n",
    "      memo[j][u] = max_prob\n",
    "      parent_arr[j][u] = max_v\n",
    "  \n",
    "  # Termination step\n",
    "\n",
    "  max_prob = -np.inf\n",
    "  max_v = None\n",
    "  for tag in tags:\n",
    "    prob = memo[n-1][tag] + transmission.get((tag, \"STOP\"))\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_v = tag\n",
    "\n",
    "  if max_prob != -np.inf:\n",
    "    memo[n]['STOP'] = max_prob\n",
    "    parent_arr[n]['STOP'] = max_v\n",
    "    \n",
    "  most_likely_sequence = [\"\" for _ in range(n)]\n",
    "  if max_v == None:\n",
    "        max_v = \"O\"\n",
    "  # Backtrack to find the most likely path\n",
    "  for t in range(n , 0, -1):\n",
    "    max_v = parent_arr[t].get(max_v)\n",
    "    if max_v == None:\n",
    "      max_v = \"O\"\n",
    "    most_likely_sequence[t-1] = max_v\n",
    "\n",
    "  return most_likely_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_loop(data, transmission, emission, emission_char, chars, ls_of_words):\n",
    "  results =[]\n",
    "  for document in data:\n",
    "    results.append(viterbi(document, transmission, emission, emission_char, chars, ls_of_words))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the prediction from trained data into the dev.in file and output\n",
    "def assign_prediction(prediction, data, path):\n",
    "    if (len(prediction) != len(data)):\n",
    "        return \"Error, prediction length != data length\"\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        assert( len(prediction[i])== len(data[i]))\n",
    "        m = len(data[i])\n",
    "        for j in range(m):\n",
    "            file.write(data[i][j] + \" \" + prediction[i][j] + \"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_part_2(dev_in_data_path, train_data_path, output_path):\n",
    "\n",
    "  #sort train data into tag and words\n",
    "  train_data = read_train_data(train_data_path)\n",
    "  train_words, train_tags = split_words_tags(train_data)\n",
    "  train_ls_of_tags = count_unique_tags(train_tags)\n",
    "  train_ls_of_words = count_unique_words(train_words)\n",
    "\n",
    "  # # Get Emission Parameters\n",
    "  k = 1\n",
    "  emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)\n",
    "\n",
    "  # Attempting to solve OOV problem with character embeddings\n",
    "\n",
    "  train_data = read_train_data(train_data_path)\n",
    "  train_char, train_tags, word_length = split_character_tags(train_data)\n",
    "  train_unqiue_ls_of_tags = count_unique_tags(train_tags)\n",
    "  train_unqiue_ls_of_char = count_unique_characters(train_char)\n",
    "\n",
    "  # Get Emmission Parameters\n",
    "  k = 1\n",
    "  emission_params_char = get_emission_parameters_char(train_unqiue_ls_of_char,train_unqiue_ls_of_tags, train_tags, train_char, k)\n",
    "\n",
    "  #append start and stop to train data per document\n",
    "  train_data_modified = read_train_data_p2(train_data_path)\n",
    "  train_words_modified, train_tags_modified = split_words_tags(train_data_modified)\n",
    "  train_ls_of_tags_modified = count_unique_tags(train_tags_modified)\n",
    "  transmission_params = get_transmission_parameters(train_ls_of_tags_modified, train_tags_modified)\n",
    "\n",
    "  #log transmission and emission params to avoid underflow\n",
    "  log_emission = log_underflow_prevention(emission_params)\n",
    "  log_emission_char = log_underflow_prevention(emission_params_char)\n",
    "  log_transmission = log_underflow_prevention(transmission_params)\n",
    "  # read dev_in in list of list\n",
    "  dev_in_list = read_dev(dev_in_data_path)\n",
    "  \n",
    "  # run viterbi and get predictions for the whole dev_in\n",
    "  predictions = viterbi_loop(dev_in_list, log_transmission, log_emission, log_emission_char, train_unqiue_ls_of_char, train_ls_of_words)\n",
    "  # write predictions into dev.p2.out\n",
    "  assign_prediction(predictions, dev_in_list, output_path)\n",
    "\n",
    "def calculate_part_2(dev_in_data_path, train_data_path, output_path):\n",
    "  write_predictions_part_2(dev_in_data_path, train_data_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and predict for part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: Output to dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdath of the data\n",
    "#------------------------------------\n",
    "# Spanish: ES\n",
    "ES_dev_in_data_path = os.path.join(\"Data\", \"ES\" , \"dev.in\")\n",
    "ES_dev_out_data_path = os.path.join(\"Data\", \"ES\" , \"dev.out\")\n",
    "ES_train_data_path = os.path.join(\"Data\", \"ES\" , \"train\")\n",
    "\n",
    "# Russiadn: RU\n",
    "RU_dev_in_data_path = os.path.join(\"Data\", \"RU\" , \"dev.in\")\n",
    "RU_dev_out_data_path = os.path.join(\"Data\", \"RU\" , \"dev.out\")\n",
    "RU_train_data_path = os.path.join(\"Data\", \"RU\" , \"train\")\n",
    "#------------------------------------\n",
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p4.out\")\n",
    "calculate_part_2(ES_dev_in_data_path, ES_train_data_path, output_path)\n",
    "print( f\"Completed for Spanish, output path is : {output_path}\")\n",
    "\n",
    "print( \"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print( \"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"dev.p4.out\")\n",
    "calculate_part_2(RU_dev_in_data_path, RU_train_data_path, output_path)\n",
    "print( f\"Completed for Russian, output path is : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Output to test.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdath of the data\n",
    "#------------------------------------\n",
    "# Spanish: ES\n",
    "ES_dev_in_data_path = os.path.join(\"Data\", \"ES\" , \"test.in\")\n",
    "ES_dev_out_data_path = os.path.join(\"Data\", \"ES\" , \"dev.out\")\n",
    "ES_train_data_path = os.path.join(\"Data\", \"ES\" , \"train\")\n",
    "\n",
    "# Russiadn: RU\n",
    "RU_dev_in_data_path = os.path.join(\"Data\", \"RU\" , \"test.in\")\n",
    "RU_dev_out_data_path = os.path.join(\"Data\", \"RU\" , \"dev.out\")\n",
    "RU_train_data_path = os.path.join(\"Data\", \"RU\" , \"train\")\n",
    "#------------------------------------\n",
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"test.p4.out\")\n",
    "calculate_part_2(ES_dev_in_data_path, ES_train_data_path, output_path)\n",
    "print( f\"Completed for Spanish, output path is : {output_path}\")\n",
    "\n",
    "print( \"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print( \"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"test.p4.out\")\n",
    "calculate_part_2(RU_dev_in_data_path, RU_train_data_path, output_path)\n",
    "print( f\"Completed for Russian, output path is : {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007ML-OdyoVhM6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
